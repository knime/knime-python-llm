# -*- coding: utf-8 -*-
# ------------------------------------------------------------------------
#  Copyright by KNIME AG, Zurich, Switzerland
#  Website: http://www.knime.com; Email: contact@knime.com
#
#  This program is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License, Version 3, as
#  published by the Free Software Foundation.
#
#  This program is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program; if not, see <http://www.gnu.org/licenses>.
#
#  Additional permission under GNU GPL version 3 section 7:
#
#  KNIME interoperates with ECLIPSE solely via ECLIPSE's plug-in APIs.
#  Hence, KNIME and ECLIPSE are both independent programs and are not
#  derived from each other. Should, however, the interpretation of the
#  GNU GPL Version 3 ("License") under any applicable laws result in
#  KNIME and ECLIPSE being a combined program, KNIME AG herewith grants
#  you the additional permission to use and propagate KNIME together with
#  ECLIPSE with only the license terms in place for ECLIPSE applying to
#  ECLIPSE and the GNU GPL Version 3 applying for KNIME, provided the
#  license terms of ECLIPSE themselves allow for the respective use and
#  propagation of ECLIPSE together with KNIME.
#
#  Additional permission relating to nodes for KNIME that extend the Node
#  Extension (and in particular that are based on subclasses of NodeModel,
#  NodeDialog, and NodeView) and that only interoperate with KNIME through
#  standard APIs ("Nodes"):
#  Nodes are deemed to be separate and independent programs and to not be
#  covered works.  Notwithstanding anything to the contrary in the
#  License, the License does not apply to Nodes, you are not required to
#  license Nodes under the License, and you are granted a license to
#  prepare and propagate Nodes, in each case even if such Nodes are
#  propagated with or for interoperation with KNIME.  The owner of a Node
#  may freely choose the license terms applicable to such Node, including
#  when such Node is propagated with or for interoperation with KNIME.
# ------------------------------------------------------------------------

from typing import Optional
import knime.extension as knext

from ..base import (
    ChatModelPortObject,
    ChatModelPortObjectSpec,
    _assert_tool_title_openai_compatibility,
    model_category,
    chat_model_port_type,
    _get_output_format_value_switch,
    _validate_json_output_format,
    _initialize_model,
)
import util


def _is_history_present(ctx: knext.DialogCreationContext) -> bool:
    specs = ctx.get_input_specs()
    return len(specs) > 1 and specs[1] is not None


def _is_tool_table_present(ctx: knext.DialogCreationContext) -> bool:
    specs = ctx.get_input_specs()
    return len(specs) > 2 and specs[2] is not None


@knext.node(
    "LLM Chat Prompter",
    knext.NodeType.PREDICTOR,
    "icons/generic/brain.png",
    model_category,
    keywords=[
        "GenAI",
        "Gen AI",
        "Generative AI",
        "Large Language Model",
        "Chat Model",
        "Message",
    ],
)
@knext.input_port("Chat Model", "A chat model.", chat_model_port_type)
@knext.input_table(
    "Conversation History",
    "An optional table containing the conversation history in a single Message column.",
    optional=True,
)
@knext.input_table(
    "Tool Definitions",
    "An optional table providing a set of tools the model can decide to call.",
    optional=True,
)
@knext.output_table(
    "Updated Conversation",
    "A table containing the full or partial conversation with the newest messages.",
)
class ChatPrompter:
    """
    Interact with a chat model within a continuous conversation.

    This node prompts a chat model with the provided user message.

    An optional table containing a **Message** column representing the conversation history
    can be provided. Existing messages will be used as context, and new messages generated by
    the model are appended to the conversation by default.

    An optional table containing a JSON column with tool definitions can be provided to enable tool calling.

    ---

    **Conversation history**:

    The conversation history table will be used as context when sending the new message to the chat model.
    To use only the conversation history table for prompting (without a new message), leave the new message setting empty and
    ensure that the last entry in the table is from either `User` or `Tool`.

    ---

    **Tool use**:

    In order to enable tool calling, a table containing tool definitions must be connected to the corresponding optional
    input port of the node.

    If the model decides to call a tool, the node appends a new 'AI' message containing said tool call.
    Information like Tool Call ID and Tool Call Arguments, normally necessary for processing a tool call,
    can be extracted from the message using the **Message Part Extractor** node. This can then be used
    to route the downstream portion of the workflow appropriately.

    The output of the tool should then be turned into a "Tool" message, appended to the conversation, and
    fed back into the node. It is crucial to ensure that this "Tool" message has the same Tool Call ID as the
    request it is responding to, which allows the model to link the two.

    During the next node execution, the model will use the messages in the conversation as context,
    including the original "User" request, the "AI" response containing the tool call, and the
    corresponding "Tool" message, to generate the final "AI" message, thus completing the tool-calling loop.

    A **tool definition** is a JSON object describing the corresponding tool and its parameters. The more
    descriptive the definition, the more likely the LLM to call it appropriately.

    Example:

    ```
    {
        "title": "number_adder",
        "type": "object",
        "description": "Adds two numbers.",
        "properties": {
            "a": {
                "title": "A",
                "type": "integer",
                "description": "First value to add"
            },
            "b": {
                "title": "B",
                "type": "integer",
                "description": "Second value to add"
            }
        },
        "required": ["a", "b"]
    }
    ```
    """

    system_message = knext.MultilineStringParameter(
        "System message",
        """
        Optional instructional message for the model at the start of the conversation,
        defining rules and persona. Also known as "developer message".
        """,
        default_value="",
    )

    chat_message = knext.MultilineStringParameter(
        "New message",
        "The new message to send to the chat model. A corresponding 'human' message will be appended to the conversation.",
        default_value="",
    )

    output_format = _get_output_format_value_switch()

    message_column = knext.ColumnParameter(
        "Message column",
        "Select the column containing the conversation history (must be of type Message).",
        port_index=1,
        column_filter=lambda c: c.ktype == util.message_type(),
    ).rule(
        knext.DialogContextCondition(_is_history_present),
        knext.Effect.SHOW,
    )

    conversation_column_name = knext.StringParameter(
        "Conversation column name",
        "The name of the new output column that will contain the conversation messages.",
        default_value="Conversation",
    ).rule(
        knext.DialogContextCondition(lambda ctx: not _is_history_present(ctx)),
        knext.Effect.SHOW,
    )

    tool_definition_column = knext.ColumnParameter(
        "Tool definition column",
        "Select the column from the 'Tool Definitions' table containing the tool definitions (JSON).",
        port_index=2,
        column_filter=util.create_type_filter(knext.logical(dict)),
    ).rule(knext.DialogContextCondition(_is_tool_table_present), knext.Effect.SHOW)

    extend_existing_conversation = knext.BoolParameter(
        "Extend existing conversation",
        """If enabled, new messages will be appended to the input conversation table.
         If disabled, the output table will only contain the new message and the model's reply.""",
        default_value=True,
    ).rule(
        knext.DialogContextCondition(_is_history_present),
        knext.Effect.SHOW,
    )

    ignore_new_message_when_tool_calling = knext.BoolParameter(
        """Ignore "New message" during tool calling""",
        """If the last message in the conversation history is a "Tool" message, ignore the "New message"
        setting specified in the dialog and let the model process the tool call result instead.""",
        default_value=True,
    ).rule(knext.DialogContextCondition(_is_tool_table_present), knext.Effect.SHOW)

    def configure(
        self,
        ctx: knext.ConfigurationContext,
        chat_model_spec: ChatModelPortObjectSpec,
        input_table_spec: Optional[knext.Schema],
        tool_table_spec: Optional[knext.Schema],
    ) -> knext.Schema:
        has_history = input_table_spec is not None
        has_tools = tool_table_spec is not None

        if has_history:
            if self.message_column:
                util.check_column(
                    input_table_spec,
                    self.message_column,
                    util.message_type(),
                    "message",
                    "Conversation History table",
                )
            else:
                self.message_column = util.pick_default_column(
                    input_table_spec, util.message_type()
                )

        if has_tools:
            if not chat_model_spec.supports_tools:
                raise knext.InvalidParametersError(
                    "The connected model does not support tool calling."
                )
            if self.tool_definition_column:
                util.check_column(
                    tool_table_spec,
                    self.tool_definition_column,
                    knext.logical(dict),
                    "tool definition",
                    "Tool Definitions table",
                )
            else:
                self.tool_definition_column = util.pick_default_column(
                    tool_table_spec, knext.logical(dict)
                )

        _validate_json_output_format(
            self.output_format, [self.system_message + self.chat_message]
        )
        chat_model_spec.validate_context(ctx)

        output_message_col_name = (
            self.message_column if has_history else self.conversation_column_name
        )

        return knext.Schema.from_columns(
            [knext.Column(util.message_type(), output_message_col_name)]
        )

    def execute(
        self,
        ctx: knext.ExecutionContext,
        chat_model: ChatModelPortObject,
        input_table: Optional[knext.Table],
        tool_table: Optional[knext.Table],
    ):
        import langchain_core.messages as lcm
        import pandas as pd
        from knime.types.message import to_langchain_message

        has_history = input_table is not None
        has_tools = tool_table is not None

        if not has_history and not self.chat_message:
            raise knext.InvalidParametersError(
                "You must specify the initial message when starting a new conversation."
            )

        conversation_messages = []
        if self.system_message:
            conversation_messages.append(lcm.SystemMessage(content=self.system_message))

        history_df = None
        is_last_message_from_tool = False
        if has_history:
            history_df = input_table.to_pandas()[[self.message_column]].copy()
            knime_messages = history_df[self.message_column].tolist()
            langchain_history = [
                to_langchain_message(msg) for msg in knime_messages if pd.notna(msg)
            ]
            conversation_messages.extend(langchain_history)

            is_last_message_from_tool = isinstance(
                conversation_messages[-1], lcm.ToolMessage
            )

        human_message = None
        should_ignore_new_message = (
            is_last_message_from_tool and self.ignore_new_message_when_tool_calling
        )
        if self.chat_message and not should_ignore_new_message:
            human_message = lcm.HumanMessage(content=self.chat_message)
            conversation_messages.append(human_message)

        chat = _initialize_model(chat_model, ctx, self.output_format)
        if has_tools:
            tool_df = tool_table.to_pandas()
            tools = tool_df[self.tool_definition_column].dropna().tolist()
            for tool_def in tools:
                _assert_tool_title_openai_compatibility(tool_def)
            chat = chat.bind_tools(tools)

        try:
            answer = chat.invoke(conversation_messages)
        except Exception as e:
            if getattr(e, "param", None) == "messages[0].role" and "system" in str(e):
                raise ValueError(
                    "The selected model does not support system messages. Please remove the system message."
                ) from e
            raise e

        response_df = self._create_response_dataframe(
            has_history, human_message, answer
        )

        if has_history and self.extend_existing_conversation:
            output_df = pd.concat([history_df, response_df], ignore_index=True)
        else:
            output_df = response_df

        return knext.Table.from_pandas(output_df)

    def _create_response_dataframe(
        self,
        has_history: bool,
        human_message,
        ai_message,
    ):
        from knime.types.message import from_langchain_message
        import pandas as pd

        column_name = (
            self.message_column if has_history else self.conversation_column_name
        )

        rows = []
        if human_message:
            rows.append({column_name: from_langchain_message(human_message)})
        rows.append({column_name: from_langchain_message(ai_message)})

        return pd.DataFrame(rows)
